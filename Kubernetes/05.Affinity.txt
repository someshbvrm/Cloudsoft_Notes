node affinity rules / node selector / pod affinity rules  #For scheduling
taints and tollerations	#For not scheduling
PodDisruptionBudgets (PDB)


Node Maintenance: Drain Command

Command: kubectl drain <node-name>

Explanation: The kubectl drain command safely evicts all pods from a Kubernetes node, marking it as unschedulable. This is typically used before performing maintenance (e.g., kernel upgrades, reboots) on the node. By default, it respects PodDisruptionBudgets and gracefully terminates pods. Critical pods (e.g., DaemonSets) are not evicted unless forced with --ignore-daemonsets. 

After maintenance, use 

kubectl uncordon <node-name> to mark the node as schedulable again.


For examples In our cluster there are 10 nodes. I want to choose only 5 nodes for my pods

Labels to nodes and nodeselector or node affinity rules in pods

Attach labels to nodes

kubectl label nodes <your-node-name> disktype=ssd

kubectl label nodes <your-node-name> project=itcm

kubectl label nodes <your-node-name> project=boa

We have attached 3 labels

kubectl get nodes --show-labels

--------------------------------------------------------------------------------
üîπNodeselector: Schedules the Pod onto any node that has a specific label

apiVersion: v1
kind: Pod
metadata:
    name: nginx
    labels:
        env: test
spec:
    containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent
    nodeSelector:
        disktype: ssd
----------------------------------------------------------------------------------

üîπBased on Node name: Forces the Pod to run on a specific node by name (foo-node).

apiVersion: v1
kind: Pod
metadata:
    name: nginx
spec:
    nodeName: foo-node # schedule pod to specific node
    containers:
    - name: nginx
      image: nginx
      imagePullPolicy: IfNotPresent

------------------------------------------------------------------------------------

1. What is Node Affinity?

Node Affinity is a set of rules used by the scheduler to decide on which nodes a pod can be placed.

Works based on node labels (key-value pairs assigned to nodes).

Similar to nodeSelector, but more expressive and flexible.

2. Types of Node Affinity

requiredDuringSchedulingIgnoredDuringExecution

	Hard rule ‚Üí Pod must be scheduled on nodes that satisfy the condition.

	If no node matches ‚Üí Pod won‚Äôt be scheduled.

preferredDuringSchedulingIgnoredDuringExecution

	Soft rule ‚Üí Scheduler tries to place pod on matching nodes.

	If not possible ‚Üí Pod will still run on a non-matching node.

‚ö†Ô∏è Both types are IgnoredDuringExecution, meaning if node labels change after scheduling, the pod won‚Äôt move automatically.


For example there are 10 servers

5 are high end and 5 are low end

when requiredDuringSchedulingIgnoredDuringExecution is set then Schedules highend pods, if not available then pods won't be scheduled
when preferredDuringSchedulingIgnoredDuringExecution is set then Schedules highend pods, if not available then low end pods be scheduled

-------------------------------------------------------------------------------------

pods/pod-with-node-affinity.yaml

apiVersion: v1
kind: Pod
metadata:
  name: with-node-affinity
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: topology.kubernetes.io/zone
            operator: In
            values:
            - antarctica-east1
            - antarctica-west1
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: another-node-label-key
            operator: In
            values:
            - another-node-label-value
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:3.8

-----------------------------------------------------------------------------------

In the above example, there are two types of affinity rules applied. The first is a hard requirement (requiredDuringSchedulingIgnoredDuringExecution). This rule states that the pod can only be scheduled on nodes that have a label called topology.kubernetes.io/zone, and that label‚Äôs value must be either antarctica-east1 or antarctica-west1. If no nodes in the cluster have one of those labels, the pod will remain unscheduled and stay in a Pending state.

The second type is a soft preference (preferredDuringSchedulingIgnoredDuringExecution). This is not mandatory, but if there are multiple nodes that satisfy the hard requirement, the scheduler will give preference to nodes that also have another label with the key another-node-label-key and the value another-node-label-value. This preference has a weight, which determines how strongly the scheduler should favor such nodes. If no node has this additional label, the pod will still be scheduled on a node that satisfies the hard rule.

Finally, the container defined in the pod is simply the Kubernetes pause image. This image doesn‚Äôt run any real application‚Äîit‚Äôs mostly used for testing or as a placeholder. The purpose here is to demonstrate how node affinity affects scheduling without worrying about application logic.

üëâ In summary, your pod can only run on nodes in the specified zones (that‚Äôs the hard requirement), and if possible, it will be placed on nodes with a certain extra label (that‚Äôs the soft preference). If no nodes meet the hard rule, the pod won‚Äôt run at all.

---------------------------------------------------------------------------------

Node affinity weight

You can specify a weight between 1 and 100 for each instance of the preferredDuringSchedulingIgnoredDuringExecution affinity type. When the scheduler finds nodes that meet all the other scheduling requirements of the Pod, the scheduler iterates through every preferred rule that the node satisfies and adds the value of the weight for that expression to a sum.

The final sum is added to the score of other priority functions for the node. Nodes with the highest total score are prioritized when the scheduler makes a scheduling decision for the Pod.

For example, consider the following Pod spec:

pods/pod-with-affinity-preferred-weight.yaml

apiVersion: v1
kind: Pod
metadata:
  name: with-affinity-preferred-weight
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/os
            operator: In
            values:
            - linux
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: label-1
            operator: In
            values:
            - key-1
      - weight: 50
        preference:
          matchExpressions:
          - key: label-2
            operator: In
            values:
            - key-2
  containers:
  - name: with-node-affinity
    image: registry.k8s.io/pause:3.8

----------------------------------------------------------------

Pod Affinity Example

pods/pod-with-pod-affinity.yaml

apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone
  containers:
  - name: with-pod-affinity
    image: registry.k8s.io/pause:3.8

---------------------------------------------------------------


Taints and Tolerations

Node affinity is a property of Pods that attracts them to a set of nodes (either as a preference or a hard requirement). Taints are the opposite -- they allow a node to repel a set of pods.

Tolerations are applied to pods. Tolerations allow the scheduler to schedule pods with matching taints. Tolerations allow scheduling but don't guarantee scheduling: the scheduler also evaluates other parameters as part of its function.

Taints and tolerations work together to ensure that pods are not scheduled onto inappropriate nodes. One or more taints are applied to a node; this marks that the node should not accept any pods that do not tolerate the taints.

üîπ Effects of Taints

There are three taint effects:

NoSchedule ‚Üí New pods will not be scheduled unless they tolerate the taint. Existing pods stay.
PreferNoSchedule ‚Üí Scheduler tries to avoid the node but will use it if no other option.
NoExecute ‚Üí New pods won‚Äôt schedule, and existing pods will be evicted if they don‚Äôt tolerate the taint.

Syntax
kubectl taint nodes node1 key1=value1:NoSchedule

kubectl taint nodes node1 reserved:itmc:NoSchedule

This command marks node1 as a reserved node. No normal pod will be scheduled there. Only pods that explicitly say ‚ÄúI tolerate reserved=itmc taint‚Äù in their configuration will be allowed. This is useful when you want to dedicate nodes for special workloads, such as system pods, monitoring agents, or high-priority apps.

------------------------------------------------------
pods/pod-with-toleration.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    env: test
spec:
  containers:
  - name: nginx
    image: nginx
    imagePullPolicy: IfNotPresent
  tolerations:
  - reserved: "itcm"
    operator: "Exists"
    effect: "NoSchedule"

--------------------------------------------------

üîπ PodDisruptionBudget (PDB)

A PodDisruptionBudget (PDB) in Kubernetes is a policy object that helps control how many pods of a certain application can be taken down voluntarily at the same time. Its purpose is to ensure high availability during planned maintenance events such as node upgrades, cluster scaling, or manual draining of nodes.

Normally, when you drain a node or perform upgrades, Kubernetes may try to evict pods. If too many pods from the same application are evicted at once, the application can experience downtime. A PDB sets limits to prevent that.

You can configure a PDB in two main ways:

minAvailable ‚Üí The minimum number of pods that must remain running at all times.

maxUnavailable ‚Üí The maximum number of pods that can be disrupted (evicted) at once.

For example, if you have a Deployment running 5 replicas of a web application, and you define a PDB with minAvailable=4, Kubernetes will ensure that at least 4 pods are always running, meaning only one pod can be disrupted at any given time. Similarly, if you set maxUnavailable=1, then only one pod can be evicted at a time.

It‚Äôs important to note that PDBs only apply to voluntary disruptions (like upgrades, drains, and scaling). They do not protect against involuntary disruptions such as node crashes or hardware failures.

üëâ In short: A PDB is a safeguard that maintains a minimum level of availability for your application during voluntary disruptions, by controlling how many pods can be evicted simultaneously.

-------------------------------------------------------------

Example PDB Using minAvailable:

policy/zookeeper-pod-disruption-budget-minavailable.yaml

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: zookeeper

--------------------------------------------------------------

Example PDB Using maxUnavailable:

policy/zookeeper-pod-disruption-budget-maxunavailable.yaml

apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: zk-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: zookeeper

----------------------------------------------------------------

-Create the PDB object

You can create or update the PDB object using kubectl.

kubectl apply -f mypdb.yaml

-Check the status of the PDB

kubectl get poddisruptionbudgets

