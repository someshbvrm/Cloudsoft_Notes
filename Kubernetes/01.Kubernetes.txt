üîπ Docker Swarm vs Kubernetes

Docker Swarm:
Definition: A lightweight container orchestration tool built into Docker.
Best Use: Suitable for small to medium microservices deployments.

Limitations of Docker Swarm:

‚ùå No Auto Scaling ‚Äì Cannot automatically add/remove containers based on demand.
‚ùå No Self-Healing ‚Äì If a container fails, it won‚Äôt restart automatically.
‚ùå Limited for large-scale deployments ‚Äì Works fine for smaller setups but not ideal for enterprise workloads.

Kubernetes (K8s)
Definition: A powerful, open-source container orchestration platform (think of it as the pilot or governor of containers).
Best Use: Designed for large-scale, production-grade deployments.

Key Functions of Kubernetes:
Automatic Provisioning ‚Äì Creates and manages containers automatically.
Scaling (Up/Down) ‚Äì Adjusts the number of containers based on workload.
Load Balancing ‚Äì Distributes traffic across containers for efficiency.
Self-Healing ‚Äì Restarts failed containers automatically.
Destroy ‚Äì Removes unnecessary or failed containers.
Update & Rollback ‚Äì Deploys updates and can roll back to a stable version if issues occur.

üîπ Kubernetes Architecture

Kubernetes has two main parts:
	Control Plane (Master) ‚Äì Brains of Kubernetes (manages the whole cluster).
	Nodes (previously called Minions) ‚Äì Workers that run the actual applications.

Control Plane (Master Components):
	API Server ‚Äì Acts as the entry point; receives commands (kubectl) and communicates with components.
	Scheduler ‚Äì Decides which node will run a new container (pod placement).
	Controller Manager ‚Äì Ensures the cluster‚Äôs desired state (e.g., if a pod dies, it creates a new one).
	etcd ‚Äì A distributed key-value store; stores cluster data (like configuration, state).

Node Components (Worker Components):
	Kubelet ‚Äì Agent running on each node; ensures pods are running as instructed by the master.\
	Kube-Proxy ‚Äì Handles networking and load balancing between pods and services.
	Container Runtime ‚Äì Software that runs the containers (e.g., Docker, containerd, CRI-O).
	Pods ‚Äì The smallest deployable unit; a pod contains one or more containers.

üîπ Kubernetes Command-Line Tools

kubectl ‚Äì The main CLI tool to interact with the Kubernetes cluster.
eksctl - AWS version
Example: kubectl get pods, kubectl apply -f deployment.yaml

kubeadm ‚Äì Tool to bootstrap (initialize) a Kubernetes cluster easily.
Example: kubeadm init, kubeadm join

Kubernetes Dashboard: Kubectl GUI version
Kubedns 
Namespaces: Virtual cluster

----------------------------------------------------------------------------------------------------------------

Pods: Encapsulation of Container, their network and storage managed by Kubernetes. It is the lowest level of object.
One pod may contain one or more containers.

Pods are the smallest deployable units of computing that you can create and manage in Kubernetes.

A Pod (as in a pod of whales or pea pod) is a group of one or more containers, with shared storage and network resources, and a specification for how to run the containers. A Pod's contents are always co-located and co-scheduled, and run in a shared context. A Pod models an application-specific "logical host": it contains one or more application containers which are relatively tightly coupled. In non-cloud contexts, applications executed on the same physical or virtual machine are analogous to cloud applications executed on the same logical host.

Pods in a Kubernetes cluster are used in two main ways:

    Pods that run a single container. The "one-container-per-Pod" model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container; Kubernetes manages Pods rather than managing the containers directly.


Example - single cointainer pod

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  containers:
  - name: nginx-container
    image: nginx:latest
    ports:
    - containerPort: 80

    Pods that run multiple containers that need to work together. A Pod can encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers form a single cohesive unit of service-for example, one container serving data stored in a shared volume to the public, while a separate sidecar container refreshes or updates those files. The Pod wraps these containers, storage resources, and an ephemeral network identity together as a single unit.

Example multi container pod

apiVersion: v1
kind: Pod
metadata:
name: multi-container-pod
spec:
containers:
- name: main-container
image: nginx:latest
ports:
- containerPort: 80
- name: sidecar-container
image: busybox:latest
command: ["sh", "-c", "while true; do echo 'Sidecar Container Running'; sleep 5; done"]




Pod controllers:

Types:
Deployment
Daemonset
Statefulsets

-------------------------------------------------------------

Deployment vs Daemonset vs Satefulsets

-----------------------------------------------------------
1. Deployment

Purpose: Run and manage stateless applications.
Scaling: Creates multiple replicas (e.g., web servers, APIs).
Pod Identity: Pods are interchangeable; no fixed identity.
Use Case: Web apps, REST APIs, stateless microservices.

2. DaemonSet

Purpose: Ensures one pod per node (or specific nodes).
Scaling: Automatically adds/removes pods as nodes join/leave.
Pod Identity: One pod copy per node; not focused on replicas.
Use Case: Logging agents, monitoring agents, storage drivers.

3. StatefulSet

Purpose: Run and manage stateful applications.
Scaling: Pods get unique, sticky identities (pod-0, pod-1, ...).
Pod Identity: Persistent hostname, stable network ID, and storage.
Use Case: Databases (MySQL, MongoDB), Kafka, Zookeeper.

--------------------------------------------------------------------


üëâKubernetes scalability limits
Max no of pods per node - 110
Max not of nodes per cluster - 5000
Max no of pods per cluster - 150000
Max no of containers per cluster - 300000

------------------------------------------------------------------------------------

Kubernetes Deployment

Definition:

A Deployment is a Kubernetes object used to manage stateless applications.
Ensures desired number of pod replicas are running and supports rolling updates and rollbacks.

Purpose:
Automates pod creation, scaling, and updates.
Maintains high availability and self-healing.

Key Fields in Deployment YAML:
apiVersion: apps/v1 ‚Äì API version for deployment.
kind: Deployment ‚Äì Resource type.
metadata ‚Äì Name, labels, and namespace.

spec:
replicas ‚Äì Number of pod replicas.
selector ‚Äì Label selector for pods.
template: ‚Äì Pod specification (containers, image, ports, env vars).
strategy: ‚Äì Update strategy (RollingUpdate or Recreate).

Common Commands:
kubectl create deployment <name> --image=<image>   # Create deployment
kubectl apply -f deployment.yaml                  # Apply or update deployment
kubectl get deployments                           # List deployments
kubectl describe deployment <name>               # Detailed view
kubectl scale deployment <name> --replicas=3     # Scale replicas
kubectl rollout status deployment <name>         # Check rollout status
kubectl rollout undo deployment <name>           # Rollback deployment

------------------------------------------------------------------------------------

Deployment example: my-nginx-deployment.yml

-----------------------------------------------------------------------------------

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: web-container
        image: nginx:1.14.2
        ports:
        - containerPort: 80

To apply this deployment in ictm-dev namespace
kubectl apply -f nginx-deployment.yml --namespace=ictm-dev

The above command creates 1 deployment, 1 replicaset and 3 pods

kubectl get deployments
kubectl get deployments --namespace=itcm-dev
kubectl get rs --namespace=itcm-dev
kubectl get pods --namespace=itcm-dev

Having 3 pods in replicaset is good practice in production as if one pod is down, traffic is still served.

To update the container image in a Deployment
kubectl set image deployment/nginx-deployment nginx=nginx:1.16.1

To change the replicas from 3 to 10
kubectl scale deployment/nginx-deployment --replicas=10


To autoscale from 10 replicas to 15 replicas if cpu usage has reached 80%
kubectl autoscale deployment/nginx-deployment  --min=10 --max=15 --cpu-percent=80

--------------------------------------------------------------------------------------------------

Creating replicaset which is not necessary. We use Deployment instead, and define applicaton in the spec section.

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # modify replicas according to your case
  replicas: 3
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google_samples/gb-frontend:v3

--------------------------------------------------------------------------------------------------------

Daemonset:

A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.

Some typical uses of a DaemonSet are:

    running a cluster storage daemon on every node
    running a logs collection daemon on every node
    running a node monitoring daemon on every node
    running cni deamon on every node
For example, controllers/daemonset.yaml file below describes a DaemonSet that runs the fluentd-elasticsearch Docker image:

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template:
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      # these tolerations are to have the daemonset runnable on control plane nodes
      # remove them if your control plane nodes should not run pods
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: quay.io/fluentd_elasticsearch/fluentd:v5.0.1
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
      # it may be desirable to set a high priority class to ensure that a DaemonSet Pod
      # preempts running Pods
      # priorityClassName: important
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log

------------------------------------------------------------------------------------------

Services:
ClusterIP
NodePort
Loadbalancer
External Name
Headless service

Ingress

The available type values and their behaviors are:

ClusterIP
Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default that is used if you don't explicitly specify a type for a Service. You can expose the Service to the public internet using an Ingress or a Gateway.

NodePort
Exposes the Service on each Node's IP at a static port (the NodePort). To make the node port available, Kubernetes sets up a cluster IP address, the same as if you had requested a Service of type: ClusterIP.

Using a NodePort gives you the freedom to set up your own load balancing solution, to configure environments that are not fully supported by Kubernetes, or even to expose one or more nodes' IP addresses directly.

LoadBalancer
Exposes the Service externally using an external load balancer. Kubernetes does not directly offer a load balancing component; you must provide one, or you can integrate your Kubernetes cluster with a cloud provider.

ExternalName
Maps the Service to the contents of the externalName field (for example, to the hostname api.foo.bar.example). The mapping configures your cluster's DNS server to return a CNAME record with that external hostname value. No proxying of any kind is set up.

| Type             | Scope/Access               | Typical Use Case                         |
| ---------------- | -------------------------- | ---------------------------------------- |
|   ClusterIP      | Internal only              | Backend services, inter-pod comms        |
|   NodePort       | External (NodeIP\:Port)    | Simple external access, dev/test         |
|   LoadBalancer   | External (Public IP)       | Production external services             |
|   ExternalName   | Maps to external DNS       | Use external DB, external API            |
|   Headless       | No ClusterIP (DNS per pod) | StatefulSets, direct pod addressing      |
|   Ingress        | HTTP/HTTPS routing         | Websites, APIs, multiple services access |
--------------------------------------------------------------------------------------------

Reference: https://devtron.ai/blog/understanding-kubernetes-services/

Storage:
PV and PVC
-------------------------------------------------------------------------------------------
Persistent Volume (PV)
Definition:
A cluster-wide storage resource provisioned by an admin (e.g., EBS, NFS, GCE disk).
Lifecycle: Exists independent of pods.
Types: Static (pre-created) or Dynamic (created via StorageClass).

Access Modes:
ReadWriteOnce (RWO) ‚Üí Mounted by a single node.
ReadOnlyMany (ROX) ‚Üí Read-only by many nodes.
ReadWriteMany (RWX) ‚Üí Read/write by many nodes.

Persistent Volume Claim (PVC)
Definition:
A request for storage by a user/application.
Binding: Kubernetes matches a PVC to a suitable PV.
Dynamic Provisioning: If no PV exists, PVC + StorageClass can provision a new PV automatically.

| Feature          | Persistent Volume (PV)                      | Persistent Volume Claim (PVC)      |
| ---------------- | ------------------------------------------- | ---------------------------------- |
| **Who creates?** | Cluster Admin (or dynamic via StorageClass) | User/Developer                     |
| **What is it?**  | Actual storage resource                     | Request for storage                |
| **Scope**        | Cluster-level object                        | Namespace-level object             |
| **Lifecycle**    | Independent of pods                         | Bound to PV until released/deleted |
| **Example**      | AWS EBS volume, NFS share                   | ‚ÄúI need 5Gi storage with RWO mode‚Äù |

PV and PVC Examples


1. Persistent Volume (PV)

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-example
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"

2. Persistent Volume Claim (PVC)

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-example
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi

3. Pod using PVC

apiVersion: v1
kind: Pod
metadata:
  name: pod-using-pvc
spec:
  containers:
  - name: app
    image: nginx
    volumeMounts:
    - mountPath: "/usr/share/nginx/html"
      name: storage
  volumes:
  - name: storage
    persistentVolumeClaim:
      claimName: pvc-example

üëâ In short:
PV = actual storage (admin-defined)
PVC = request for storage (user-defined)
Pod = uses PVC to consume PV

------------------------------------------------------------------

Stateless application - Deployment (front end microservices)
Stateful application - Statefulset (Backend microservices like Databases, nosql, Monitoring agents, log collection agents, storage nodes, cni - Daemonset)

StatefulSet is the workload API object used to manage stateful applications.

Manages the deployment and scaling of a set of Pods, and provides guarantees about the ordering and uniqueness of these Pods.

Like a Deployment, a StatefulSet manages Pods that are based on an identical container spec. Unlike a Deployment, a StatefulSet maintains a sticky identity for each of its Pods. These pods are created from the same spec, but are not interchangeable: each has a persistent identifier that it maintains across any rescheduling.

Using StatefulSets
StatefulSets are valuable for applications that require one or more of the following.

    Stable, unique network identifiers.
    Stable, persistent storage.
    Ordered, graceful deployment and scaling.
    Ordered, automated rolling updates.

